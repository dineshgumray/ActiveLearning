{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AL_POC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshgumray/ActiveLearning/blob/main/AL_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install small-text[transformers]  # use \"small-text\" without \"[transformers]\" if you want to work on the CPU only\n",
        "%pip install datasets"
      ],
      "metadata": {
        "id": "WD-swpbz0GHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjHlKhq4zYIZ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# disables the progress bar for notebooks: https://github.com/huggingface/datasets/issues/2651\n",
        "datasets.logging.get_verbosity = lambda: logging.NOTSET\n",
        "\n",
        "raw_dataset = datasets.load_dataset('banking77')\n",
        "num_classes = np.unique(raw_dataset['train']['label']).shape[0]\n",
        "\n",
        "print('First 10 training samples:\\n')\n",
        "for i in range(10):\n",
        "    print(raw_dataset['train']['label'][i], ' ', raw_dataset['train']['text'][i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "transformers.logging.get_verbosity = lambda: logging.NOTSET\n",
        "\n",
        "\n",
        "transformer_model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    transformer_model_name\n",
        ")"
      ],
      "metadata": {
        "id": "pC9aqkiq0lrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from small_text.integrations.transformers.datasets import TransformersDataset\n",
        "\n",
        "\n",
        "def get_transformers_dataset(tokenizer, data, labels, max_length=60):\n",
        "\n",
        "    data_out = []\n",
        "\n",
        "    for i, doc in enumerate(data):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            doc,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation='longest_first'\n",
        "        )\n",
        "\n",
        "        data_out.append((encoded_dict['input_ids'], encoded_dict['attention_mask'], labels[i]))\n",
        "\n",
        "    return TransformersDataset(data_out)\n",
        "\n",
        "\n",
        "train = get_transformers_dataset(tokenizer, raw_dataset['train']['text'], raw_dataset['train']['label'])\n",
        "test = get_transformers_dataset(tokenizer, raw_dataset['test']['text'], raw_dataset['test']['label'])"
      ],
      "metadata": {
        "id": "aOOfP1YO0uUR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from small_text.active_learner import PoolBasedActiveLearner\n",
        "\n",
        "from small_text.initialization import random_initialization_balanced\n",
        "from small_text.integrations.transformers import TransformerModelArguments\n",
        "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n",
        "from small_text.query_strategies import PredictionEntropy\n",
        "from small_text.query_strategies import LeastConfidence\n",
        "from small_text.query_strategies import BreakingTies\n",
        "from small_text.integrations.transformers import TransformerModelArguments\n",
        "\n",
        "\n",
        "# simulates an initial labeling to warm-start the active learning process\n",
        "def initialize_active_learner(active_learner, y_train):\n",
        "    \n",
        "\n",
        "    x_indices_initial = random_initialization_balanced(y_train, n_samples=1000)\n",
        "    y_initial = y_train[x_indices_initial]\n",
        "\n",
        "    active_learner.initialize_data(x_indices_initial, y_initial)\n",
        "\n",
        "    return x_indices_initial\n",
        "\n",
        "transformer_model = TransformerModelArguments(transformer_model_name)\n",
        "clf_factory = TransformerBasedClassificationFactory(transformer_model, \n",
        "                                                    num_classes, \n",
        "                                                    kwargs=dict({\n",
        "                                                                 'mini_batch_size': 32,\n",
        "                                                                 'early_stopping_no_improvement': -1\n",
        "                                                                }))\n",
        "query_strategy = BreakingTies()\n",
        "\n",
        "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n",
        "labeled_indices = initialize_active_learner(active_learner, train.y)"
      ],
      "metadata": {
        "id": "4nyxDMzX0-F3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_labeled_indices = labeled_indices"
      ],
      "metadata": {
        "id": "99RAjEc32r5U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "w8M7yMO8WV-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/log1.txt\", \"a\")"
      ],
      "metadata": {
        "id": "nCl6rCuoSth-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_learner.save('/content/drive/MyDrive/initial_active_leaner.pkl')"
      ],
      "metadata": {
        "id": "gbhGK7n1ppg-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_learner = PoolBasedActiveLearner.load('/content/drive/MyDrive/initial_active_leaner.pkl')"
      ],
      "metadata": {
        "id": "VkgaZRActvd3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labeled_indices)"
      ],
      "metadata": {
        "id": "n7X8B0gnzLfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_strategy = LeastConfidence()\n",
        "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n",
        "print(active_learner.query_strategy)\n",
        "labeled_indices = initial_labeled_indices\n",
        "print(len(labeled_indices))\n",
        "active_learner.initialize_data(labeled_indices, train.y[labeled_indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpLd8pQPu9Zq",
        "outputId": "59bf4065-4326-47ba-b555-7f08324a9d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeastConfidence()\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "metadata": {
        "id": "PFvztGe2qi3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate(active_learner, train, test):\n",
        "    y_pred = active_learner.classifier.predict(train)\n",
        "    y_pred_test = active_learner.classifier.predict(test)\n",
        "    test_acc = accuracy_score(y_pred_test, test.y)\n",
        "    train_acc = accuracy_score(y_pred, train.y)\n",
        "\n",
        "    print(\"Train accuracy: {}\".format(train_acc))\n",
        "    print(\"Test accuracy: {}\".format(test_acc))\n",
        "    print(\"Train Report \\n\")\n",
        "    print(classification_report(train.y, y_pred))\n",
        "    print(\"Test Report \\n\")\n",
        "    print(classification_report(test.y, y_pred_test))\n",
        "    f.write(\"\\n Train Report \\n\")\n",
        "    f.write(classification_report(train.y, y_pred))\n",
        "    f.write(\"\\n Test Report \\n\")\n",
        "    f.write(classification_report(test.y, y_pred_test))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"Train accuracy: \")\n",
        "    f.write(str(train_acc))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"Test accuracy: \")\n",
        "    f.write(str(test_acc))\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "results = []\n",
        "results.append(evaluate(active_learner, train[labeled_indices], test))\n"
      ],
      "metadata": {
        "id": "0fbi1elEIk5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_queries = 5\n",
        "\n",
        "for i in range(num_queries):\n",
        "    # ...where each iteration consists of labelling 1000 samples\n",
        "    q_indices = active_learner.query(num_samples=1000)\n",
        "\n",
        "    # Simulate user interaction here. Replace this for real-world usage.\n",
        "    y = train.y[q_indices]\n",
        "\n",
        "    # Return the labels for the current query to the active learner.\n",
        "    active_learner.update(y)\n",
        "\n",
        "    labeled_indices = np.concatenate([q_indices, labeled_indices])\n",
        "\n",
        "    print('Iteration #{:d} ({} samples)'.format(i, len(labeled_indices)))\n",
        "    f.write(\"\\n############################################################\\n\")\n",
        "    f.write(\"Iteration \")\n",
        "    f.write(str(i+1))\n",
        "    f.write(\"\\t Samples \")\n",
        "    f.write(str(len(labeled_indices)))\n",
        "    results.append(evaluate(active_learner, train[labeled_indices], test))\n",
        "    \n",
        "f.close()"
      ],
      "metadata": {
        "id": "ciDNBvw73vDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_learner.save('/content/drive/MyDrive/active_leaner.pkl')"
      ],
      "metadata": {
        "id": "LTXR7HSv6VK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}